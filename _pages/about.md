---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a fifth-year Ph.D. candidate at the Institute of Artificial Intelligence and Robotics ([IAIR](https://iair.xjtu.edu.cn/index.htm), Xi'an Jiaotong University (XJTU), where I pursue my doctoral studies under the supervision of Prof.[Le Wang](https://gr.xjtu.edu.cn/web/lewang). I also collaborate closely with Prof.[Sanping Zhou](https://gr.xjtu.edu.cn/web/spzhou), Prof.[Gang Hua](https://www.ganghua.org/) and Prof.[Wei Tang](https://www.cs.uic.edu/~tangw/).

Previously, I received my B.Eng. degree in Robotics Engineering from Harbin Institute of Technology. I am currently a visiting Ph.D. student with the Multimedia and Human Understanding Group (MHUG) at the University of Trento, under the supervision of [Nicu Sebe](https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en).




# üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìù Selected Publications

<div class='paper-box'><div class='paper-box-image'><div><div class="badge">CVPR 2016</div><img src='images/500x300.png' alt="sym" width="100%"></div></div>
<div class='paper-box-text' markdown="1">

[Deep Residual Learning for Image Recognition](https://openaccess.thecvf.com/content_cvpr_2016/papers/He_Deep_Residual_Learning_CVPR_2016_paper.pdf)

**Kaiming He**, Xiangyu Zhang, Shaoqing Ren, Jian Sun

[**Project**](https://scholar.google.com/citations?view_op=view_citation&hl=zh-CN&user=DhtAFkwAAAAJ&citation_for_view=DhtAFkwAAAAJ:ALROH1vI_8AC) <strong><span class='show_paper_citations' data='DhtAFkwAAAAJ:ALROH1vI_8AC'></span></strong>
- Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
</div>
</div>

- [Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet](https://github.com), A, B, C, **CVPR 2020**

# Publications 
- **MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking**  
  <u>Qin Z</u>, Zhou S, Wang L, Duan J, Hua G, Tang W. **CVPR 2023**

- **Towards Generalizable Multi-Object Tracking**  
  <u>Qin Z</u>, Wang L, Zhou S, Fu P, Hua G, Tang W. **CVPR 2024**

- **Referencing Where to Focus: Improving Visual Grounding with Referential Query**  
  Wang Y, Tian Z, Guo Q, <u>Qin Z</u>, Zhou S, Yang M, Wang L. **NeurIPS 2024**

- **RefDetector: A Simple yet Effective Matching-based Method for Referring Expression Comprehension**  
  Wang Y, Tian Z, <u>Qin Z</u>, Zhou S, Wang L. **AAAI 2025**

- **Towards Precise Embodied Dialogue Localization via Causality Guided Diffusion**  
  Wang H, Wang L, <u>Qin Z</u>, Wang Y, Hua G, Tang W. **CVPR 2025**

- **Versatile Multimodal Controls for Whole-Body Talking Human Animation**  
  <u>Qin Z</u>, Zheng R, Wang Y, Li T, Zhu Z, Yang M, Yang M, Wang L. **ACM MM 2025**

- **HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs**  
  <u>Qin Z</u>, Zheng R, Wang Y, Li T, Yuan Y, Chen J, Wang L. **AAAI 2026**

- **Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking**  
  Li Y, Zhou S, <u>Qin Z</u>, Wang L, Wang J, Zheng N. **TMM 2024**

- **Robust Noisy Label Learning via Two-Stream Sample Distillation**  
  Bai S, Zhou S, <u>Qin Z</u>, Wang L, Zheng N. **TMM 2025**

- **Semantic and Kinematics Guidance for RMOT**  
  Li Y, Zhou S, <u>Qin Z</u>, Wang L. **TMM 2025**

- **Injecting Position and Relation Prior for Dense Video Captioning**  
  Li Y, Zhou S, <u>Qin Z</u>, Lin J, Sun X, Wu K, Wang L. *(Submitted to **TIP**)*

- **From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval**  
  Wang Y, Tian Z, Guo Q, <u>Qin Z</u>, Zhou S, Yang M, Wang L. *(Submitted to **TCSVT**)*

- **Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion**  
  <u>Qin Z</u>, Wang L, Wang Y, Yang M, Rong C, Yang M, Zheng N. *(Submitted to **TCSVT**)*

- **RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation**  
  <u>Qin Z</u>, Wang L, Wang Y, Zhou S, Hua G, Tang W. *(Submitted to **TCSVT**)*

- **Spatial Matters: Position-Guided 3D Referring Expression Segmentation**  
  Wang Y, Tian Z, Wang L, <u>Qin Z</u>, Zhou S. *(Submitted to **CVPR 2026**)*


# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.


# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2025.09 - (now)*, Visiting Ph.D. student, Artificial Intelligence, University of Trento.
- *2021.09 - (now)*, Ph.D. student, Control Science and Engineering, Xi'an Jiaotong University. 
- *2017.09 - 2021.06*, B.S., Robotics Engineering, Harbin Institute of Technology

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# Services
Reviewer for CVPR, ICCV, ICML, ECCV, ICLR, NIPS, ACM MM, AAAI, etc.
