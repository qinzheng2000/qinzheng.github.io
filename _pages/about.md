---
permalink: /
title: ""
excerpt: ""
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

{% if site.google_scholar_stats_use_cdn %}
{% assign gsDataBaseUrl = "https://cdn.jsdelivr.net/gh/" | append: site.repository | append: "@" %}
{% else %}
{% assign gsDataBaseUrl = "https://raw.githubusercontent.com/" | append: site.repository | append: "/" %}
{% endif %}
{% assign url = gsDataBaseUrl | append: "google-scholar-stats/gs_data_shieldsio.json" %}

<span class='anchor' id='about-me'></span>

I am a fifth-year Ph.D. candidate at the Institute of Artificial Intelligence and Robotics ([IAIR](https://iair.xjtu.edu.cn/index.htm), Xi'an Jiaotong University (XJTU), where I pursue my doctoral studies under the supervision of Prof.[Le Wang](https://gr.xjtu.edu.cn/web/lewang). I also collaborate closely with Prof.[Sanping Zhou](https://gr.xjtu.edu.cn/web/spzhou), Prof.[Gang Hua](https://www.ganghua.org/) and Prof.[Wei Tang](https://www.cs.uic.edu/~tangw/).

Previously, I received my B.Eng. degree in Robotics Engineering from Harbin Institute of Technology. I am currently a visiting Ph.D. student with the Multimedia and Human Understanding Group (MHUG) at the University of Trento, under the supervision of [Nicu Sebe](https://scholar.google.com/citations?user=stFCYOAAAAAJ&hl=en).




# üî• News
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2022.02*: &nbsp;üéâüéâ Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìù Selected Publications
<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Milti-model LLM</div>
    <img src='images/humansense.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs</h3>
    <div class="authors">
      <strong>Zheng Qin</strong>, Ruobing Zheng, Yabing Wang, Tianqi Li, Yi Yuan, Jingdong Chen, Le Wang
    </div>
    <div class="venue">AAAI26</div>
    <div class="links">
      <a href="https://digital-avatar.github.io/ai/HumanSense/" class="btn-accent">Project</a>
      <a href="https://arxiv.org/abs/2508.10576" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/antgroup/HumanSense" class="btn-accent"><i class="fab fa-github"></i> Code</a>
      <a href="https://huggingface.co/antgroup/HumanSense_Omni_Reasoning" class="btn-accent"> Huggingface</a>
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Video Generation</div>
    <img src='images/verse.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Versatile Multimodal Controls for Expressive Talking Human Animation</h3>
    <div class="authors">
      <strong>Zheng Qin</strong>, Ruobing Zheng, Yabing Wang, Tianqi Li, Zixin Zhu, Sanping Zhou, Ming Yang, Le Wang
    </div>
    <div class="venue">ACM MM25</div>
    <div class="links">
      <a href="https://digital-avatar.github.io/ai/VersaAnimator/" class="btn-accent">Project</a>
      <a href="https://arxiv.org/abs/2503.08714" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <!-- <a href="https://github.com/antgroup/HumanSense" class="btn-accent"><i class="fab fa-github"></i> Code</a> -->
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Multi-object Tracking</div>
    <img src='images/generaltrack.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Towards generalizable multi-object tracking</h3>
    <div class="authors">
      <strong>Zheng Qin</strong>, Le Wang, Sanping Zhou, Panpan Fu, Gang Hua, Wei Tang
    </div>
    <div class="venue">CVPR24</div>
    <div class="links">
      <!-- <a href="https://digital-avatar.github.io/ai/VersaAnimator/" class="btn-accent">Project</a> -->
      <a href="https://arxiv.org/abs/2406.00429" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://github.com/qinzheng2000/GeneralTrack" class="btn-accent"><i class="fab fa-github"></i> Code</a>
    </div>
  </div>
</div>

<div class='paper-box'>
  <div class='paper-box-image'>
    <div class="badge pulse-accent">Multi-object Tracking</div>
    <img src='images/motiontrack.png' alt="SAMPO" width="100%">
  </div>
  <div class='paper-box-text'>
    <h3>Motiontrack: Learning robust short-term and long-term motions for multi-object tracking</h3>
    <div class="authors">
      <strong>Zheng Qin</strong>, Sanping Zhou, Le Wang, Jinghai Duan, Gang Hua, Wei Tang
    </div>
    <div class="venue">CVPR23</div>
    <div class="links">
      <a href="https://www.youtube.com/watch?v=HaS9cM75J7Y" class="btn-accent">Video</a>
      <a href="https://arxiv.org/abs/2303.10404" class="btn-accent"><i class="fas fa-file-alt"></i> Paper</a>
      <a href="https://www.youtube.com/shorts/CFmHsWB_Sus" class="btn-accent"><i class="fab fa-github"></i> Demo</a>
    </div>
  </div>
</div>



# Publications 
- MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking (CVPR 2023)  
  **Qin Z**, Zhou S, Wang L, Duan J, Hua G, Tang W.

- Towards Generalizable Multi-Object Tracking (CVPR 2024)  
  **Qin Z**, Wang L, Zhou S, Fu P, Hua G, Tang W.

- Referencing Where to Focus: Improving Visual Grounding with Referential Query (NeurIPS 2024)  
  Wang Y, Tian Z, Guo Q, **Qin Z**, Zhou S, Yang M, Wang L.

- RefDetector: A Simple yet Effective Matching-based Method for Referring Expression Comprehension (AAAI 2025)  
  Wang Y, Tian Z, **Qin Z**, Zhou S, Wang L.

- Towards Precise Embodied Dialogue Localization via Causality Guided Diffusion (CVPR 2025)  
  Wang H, Wang L, **Qin Z**, Wang Y, Hua G, Tang W.

- Versatile Multimodal Controls for Whole-Body Talking Human Animation (ACM MM 2025)  
  **Qin Z**, Zheng R, Wang Y, Li T, Zhu Z, Yang M, Yang M, Wang L.

- HumanSense: From Multimodal Perception to Empathetic Context-Aware Responses through Reasoning MLLMs (AAAI 2026)  
  **Qin Z**, Zheng R, Wang Y, Li T, Yuan Y, Chen J, Wang L.

- Single-Shot and Multi-Shot Feature Learning for Multi-Object Tracking (TMM 2024)  
  Li Y, Zhou S, **Qin Z**, Wang L, Wang J, Zheng N.

- Robust Noisy Label Learning via Two-Stream Sample Distillation (TMM 2025)  
  Bai S, Zhou S, **Qin Z**, Wang L, Zheng N.

- Semantic and Kinematics Guidance for RMOT (TMM 2025)  
  Li Y, Zhou S, **Qin Z**, Wang L.

- Injecting Position and Relation Prior for Dense Video Captioning (Submitted to TIP)  
  Li Y, Zhou S, **Qin Z**, Lin J, Sun X, Wu K, Wang L.

- From Mapping to Composing: A Two-Stage Framework for Zero-shot Composed Image Retrieval (Submitted to TCSVT)  
  Wang Y, Tian Z, Guo Q, **Qin Z**, Zhou S, Yang M, Wang L.

- Embracing Aleatoric Uncertainty: Generating Diverse 3D Human Motion (Submitted to TCSVT)  
  **Qin Z**, Wang L, Wang Y, Yang M, Rong C, Yang M, Zheng N.

- RSRNav: Reasoning Spatial Relationship for Image-Goal Navigation (Submitted to TCSVT)  
  **Qin Z**, Wang L, Wang Y, Zhou S, Hua G, Tang W.

- Spatial Matters: Position-Guided 3D Referring Expression Segmentation (Submitted to CVPR 2026)  
  Wang Y, Tian Z, Wang L, **Qin Z**, Zhou S.



# üíª Internships
- *2019.05 - 2020.02*, [Lorem](https://github.com/), China.


# üéñ Honors and Awards
- *2021.10* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.09* Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 

# üìñ Educations
- *2025.09 - (now)*, Visiting Ph.D. student, Artificial Intelligence, University of Trento.
- *2021.09 - (now)*, Ph.D. student, Control Science and Engineering, Xi'an Jiaotong University. 
- *2017.09 - 2021.06*, B.S., Robotics Engineering, Harbin Institute of Technology

# üí¨ Invited Talks
- *2021.06*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet. 
- *2021.03*, Lorem ipsum dolor sit amet, consectetur adipiscing elit. Vivamus ornare aliquet ipsum, ac tempus justo dapibus sit amet.  \| [\[video\]](https://github.com/)

# Services
Reviewer for CVPR, ICCV, ICML, ECCV, ICLR, NIPS, ACM MM, AAAI, etc.



<style>
.video-slider { position: relative; max-width: 640px; margin: 1rem auto; }
.slide { display: none; }
iframe, video { width: 100%; aspect-ratio: 16/9; border: none; }
.prev, .next {
  position: absolute; top: 50%; transform: translateY(-50%);
  padding: 6px 10px; background: rgba(0,0,0,.5); color:#fff; cursor:pointer;
}
.next { right: 0; }
.dots { text-align:center; margin-top:6px; }
.dots span { display:inline-block; width:10px; height:10px;
  background:#bbb; border-radius:50%; margin:0 3px; cursor:pointer; }
.dots span.active { background:#717171; }
</style>

<script>
let slideIndex=1; showSlides(slideIndex);
function plusSlides(n){ showSlides(slideIndex+=n); }
function currentSlide(n){ showSlides(slideIndex=n); }
function showSlides(n){
  const slides=document.querySelectorAll('.slide');
  const dots=document.querySelectorAll('.dots span');
  if(n>slides.length) slideIndex=1;
  if(n<1) slideIndex=slides.length;
  slides.forEach(s=>s.style.display="none");
  dots.forEach(d=>d.classList.remove("active"));
  slides[slideIndex-1].style.display="block";
  dots[slideIndex-1].classList.add("active");
}
</script>


